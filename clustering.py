# -*- coding: utf-8 -*-
"""Clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DOc4FR6OqKvm6IzHHiMCNlN3CwQLfJE9
"""

#Wine Dataset
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_wine
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import adjusted_rand_score

# Load the Wine dataset
wine = load_wine()
X = wine.data
y = wine.target
feature_names = wine.feature_names

# Convert to DataFrame for easier data manipulation
df = pd.DataFrame(X, columns=feature_names)
df['target'] = y

# Preprocessing: Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply K-Means Clustering (3 clusters as there are 3 wine types)
kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(X_scaled)

# Add the cluster predictions to the DataFrame
df['cluster'] = clusters

# Apply PCA to reduce the dataset to 2 dimensions for visualization
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# Add PCA results to DataFrame
df['PCA1'] = X_pca[:, 0]
df['PCA2'] = X_pca[:, 1]

# Visualization: Plot the clusters
plt.figure(figsize=(10, 7))
sns.scatterplot(x='PCA1', y='PCA2', hue='cluster', palette='Set2', data=df, s=100, marker='o')
plt.title('K-Means Clustering on Wine Dataset (2D PCA Projection)')
plt.show()

# Evaluation: Compare clustering labels with true labels using Adjusted Rand Index
ari_score = adjusted_rand_score(y, clusters)
print(f"Adjusted Rand Index (ARI) Score: {ari_score:.2f}")

# Display the cluster centers in original feature space
cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)
print("\nCluster Centers (in original feature space):")
for i, center in enumerate(cluster_centers):
    print(f"Cluster {i}: {center}")

# Display a comparison of the true labels and the predicted clusters
comparison_df = pd.DataFrame({'True Label': y, 'Predicted Cluster': clusters})
print("\nComparison of True Labels vs Predicted Clusters:")
print(comparison_df.head())

# Optional: Visualize the K-means inertia to find the optimal number of clusters (Elbow Method)
inertias = []
k_values = range(1, 10)
for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_scaled)
    inertias.append(kmeans.inertia_)

# Plot the elbow curve
plt.figure(figsize=(8, 6))
plt.plot(k_values, inertias, marker='o')
plt.title('Elbow Method for Optimal K')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.show()

